{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48c099eb",
   "metadata": {},
   "source": [
    "# üéØ AI Interview Coach with LangGraph\n",
    "\n",
    "A comprehensive interview preparation system that simulates realistic interviews using multi-agent workflow orchestration.\n",
    "\n",
    "**Workflow:** research_company ‚Üí research_interviewer ‚Üí generate_questions ‚Üí interviewer_bot ‚Üî get_response ‚Üí final_report\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bafd8cee",
   "metadata": {},
   "source": [
    "## üì¶ Section 1: Setup & Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09b17c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q langgraph langchain langchain-openai langchain-anthropic tavily-python google-search-results python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ea28cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import core libraries\n",
    "import os\n",
    "import json\n",
    "import asyncio\n",
    "from getpass import getpass\n",
    "from typing import TypedDict, Annotated, Literal, Optional, List, Dict, Any\n",
    "from datetime import datetime\n",
    "\n",
    "# LangChain & LangGraph\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "# Research APIs\n",
    "from tavily import TavilyClient\n",
    "from serpapi import GoogleSearch\n",
    "\n",
    "# Colab utilities\n",
    "try:\n",
    "    from google.colab import files\n",
    "    IN_COLAB = True\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "    print(\"‚ö†Ô∏è  Not running in Colab - file export features will be limited\")\n",
    "\n",
    "print(\"‚úÖ All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13fdb3e6",
   "metadata": {},
   "source": [
    "## üîë Section 2: Configuration & API Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d88ac94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure API Keys (secure input)\n",
    "print(\"üîê Enter your API keys (input will be hidden)\\n\")\n",
    "\n",
    "# LLM Provider Keys\n",
    "OPENAI_API_KEY = getpass(\"OpenAI API Key (optional, press Enter to skip): \") or None\n",
    "ANTHROPIC_API_KEY = getpass(\"Anthropic API Key (optional, press Enter to skip): \") or None\n",
    "\n",
    "# Research API Keys\n",
    "TAVILY_API_KEY = getpass(\"Tavily API Key (optional, press Enter to skip): \") or None\n",
    "SERPAPI_KEY = getpass(\"SERP API Key (optional, press Enter to skip): \") or None\n",
    "\n",
    "# Set environment variables\n",
    "if OPENAI_API_KEY:\n",
    "    os.environ['OPENAI_API_KEY'] = OPENAI_API_KEY\n",
    "if ANTHROPIC_API_KEY:\n",
    "    os.environ['ANTHROPIC_API_KEY'] = ANTHROPIC_API_KEY\n",
    "if TAVILY_API_KEY:\n",
    "    os.environ['TAVILY_API_KEY'] = TAVILY_API_KEY\n",
    "if SERPAPI_KEY:\n",
    "    os.environ['SERPAPI_API_KEY'] = SERPAPI_KEY\n",
    "\n",
    "print(\"\\n‚úÖ Configuration complete!\")\n",
    "print(f\"   OpenAI: {'‚úì' if OPENAI_API_KEY else '‚úó'}\")\n",
    "print(f\"   Anthropic: {'‚úì' if ANTHROPIC_API_KEY else '‚úó'}\")\n",
    "print(f\"   Tavily: {'‚úì' if TAVILY_API_KEY else '‚úó'}\")\n",
    "print(f\"   SERP API: {'‚úì' if SERPAPI_KEY else '‚úó'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4340e5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM Provider Selection\n",
    "LLM_PROVIDER = \"anthropic\"  # Options: \"openai\" or \"anthropic\"\n",
    "MODEL_NAME = \"claude-3-5-sonnet-20241022\" if LLM_PROVIDER == \"anthropic\" else \"gpt-4-turbo\"\n",
    "\n",
    "print(f\"ü§ñ Using: {LLM_PROVIDER.title()} ({MODEL_NAME})\")\n",
    "\n",
    "# Initialize LLM\n",
    "def get_llm(temperature=0.7):\n",
    "    if LLM_PROVIDER == \"anthropic\":\n",
    "        if not ANTHROPIC_API_KEY:\n",
    "            raise ValueError(\"Anthropic API key not provided\")\n",
    "        return ChatAnthropic(model=MODEL_NAME, temperature=temperature)\n",
    "    else:\n",
    "        if not OPENAI_API_KEY:\n",
    "            raise ValueError(\"OpenAI API key not provided\")\n",
    "        return ChatOpenAI(model=MODEL_NAME, temperature=temperature)\n",
    "\n",
    "# Test LLM\n",
    "try:\n",
    "    llm = get_llm()\n",
    "    print(\"‚úÖ LLM initialized successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå LLM initialization failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab08e794",
   "metadata": {},
   "source": [
    "## üìä Section 3: State Schema Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bbe3177",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the interview session state\n",
    "class InterviewState(TypedDict):\n",
    "    # Input configuration\n",
    "    company_name: str\n",
    "    role_title: str\n",
    "    job_description: Optional[str]\n",
    "    interviewer_name: Optional[str]\n",
    "    candidate_background: Optional[str]\n",
    "    seniority_level: Literal[\"junior\", \"mid\", \"senior\", \"lead\"]\n",
    "    interview_type: Literal[\"behavioral\", \"system-design\", \"coding\", \"mixed\"]\n",
    "    \n",
    "    # Research outputs\n",
    "    company_summary: Optional[str]\n",
    "    talking_points: Optional[List[str]]\n",
    "    interviewer_persona: Optional[str]\n",
    "    interviewer_hypotheses: Optional[List[str]]\n",
    "    \n",
    "    # Question generation\n",
    "    questions: List[str]\n",
    "    \n",
    "    # Interview session state\n",
    "    conversation_history: List[Dict[str, Any]]\n",
    "    current_question_index: int\n",
    "    stop_requested: bool\n",
    "    coach_mode_active: bool\n",
    "    \n",
    "    # Progress tracking\n",
    "    coverage_tracker: Dict[str, float]\n",
    "    \n",
    "    # Final output\n",
    "    final_report: Optional[str]\n",
    "\n",
    "# Initialize default state\n",
    "def create_initial_state(\n",
    "    company_name: str,\n",
    "    role_title: str,\n",
    "    seniority_level: str = \"mid\",\n",
    "    interview_type: str = \"mixed\",\n",
    "    job_description: str = None,\n",
    "    interviewer_name: str = None,\n",
    "    candidate_background: str = None\n",
    ") -> InterviewState:\n",
    "    return {\n",
    "        \"company_name\": company_name,\n",
    "        \"role_title\": role_title,\n",
    "        \"job_description\": job_description,\n",
    "        \"interviewer_name\": interviewer_name,\n",
    "        \"candidate_background\": candidate_background,\n",
    "        \"seniority_level\": seniority_level,\n",
    "        \"interview_type\": interview_type,\n",
    "        \"company_summary\": None,\n",
    "        \"talking_points\": [],\n",
    "        \"interviewer_persona\": None,\n",
    "        \"interviewer_hypotheses\": [],\n",
    "        \"questions\": [],\n",
    "        \"conversation_history\": [],\n",
    "        \"current_question_index\": 0,\n",
    "        \"stop_requested\": False,\n",
    "        \"coach_mode_active\": False,\n",
    "        \"coverage_tracker\": {\n",
    "            \"problem_solving\": 0.0,\n",
    "            \"collaboration\": 0.0,\n",
    "            \"ambiguity_handling\": 0.0,\n",
    "            \"leadership_ownership\": 0.0,\n",
    "            \"technical_depth\": 0.0\n",
    "        },\n",
    "        \"final_report\": None\n",
    "    }\n",
    "\n",
    "print(\"‚úÖ State schema defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4767cd0b",
   "metadata": {},
   "source": [
    "## üîç Section 4: Research Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08e6e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Research utility functions\n",
    "def search_with_tavily(query: str, max_results: int = 5) -> List[Dict[str, str]]:\n",
    "    \"\"\"Search using Tavily API\"\"\"\n",
    "    if not TAVILY_API_KEY:\n",
    "        return []\n",
    "    \n",
    "    try:\n",
    "        client = TavilyClient(api_key=TAVILY_API_KEY)\n",
    "        response = client.search(query, max_results=max_results)\n",
    "        return response.get('results', [])\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Tavily search failed: {e}\")\n",
    "        return []\n",
    "\n",
    "def search_with_serp(query: str, max_results: int = 5) -> List[Dict[str, str]]:\n",
    "    \"\"\"Search using SERP API as fallback\"\"\"\n",
    "    if not SERPAPI_KEY:\n",
    "        return []\n",
    "    \n",
    "    try:\n",
    "        params = {\n",
    "            \"q\": query,\n",
    "            \"api_key\": SERPAPI_KEY,\n",
    "            \"num\": max_results\n",
    "        }\n",
    "        search = GoogleSearch(params)\n",
    "        results = search.get_dict()\n",
    "        \n",
    "        organic = results.get('organic_results', [])\n",
    "        return [\n",
    "            {\"title\": r.get(\"title\", \"\"), \"url\": r.get(\"link\", \"\"), \"content\": r.get(\"snippet\", \"\")}\n",
    "            for r in organic[:max_results]\n",
    "        ]\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  SERP API search failed: {e}\")\n",
    "        return []\n",
    "\n",
    "def search_web(query: str, max_results: int = 5) -> List[Dict[str, str]]:\n",
    "    \"\"\"Search with Tavily, fallback to SERP\"\"\"\n",
    "    print(f\"üîç Searching: {query}\")\n",
    "    \n",
    "    results = search_with_tavily(query, max_results)\n",
    "    \n",
    "    if not results:\n",
    "        print(\"  ‚Üí Tavily failed, trying SERP API...\")\n",
    "        results = search_with_serp(query, max_results)\n",
    "    \n",
    "    if results:\n",
    "        print(f\"  ‚úì Found {len(results)} results\")\n",
    "    else:\n",
    "        print(\"  ‚úó No results found\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"‚úÖ Research utilities ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e8889b",
   "metadata": {},
   "source": [
    "## üè¢ Section 5: Node - Research Company"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c516dd70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def research_company_node(state: InterviewState) -> Dict[str, Any]:\n",
    "    \"\"\"Research company background, tech stack, and culture\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üè¢ RESEARCHING COMPANY\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    company = state['company_name']\n",
    "    role = state['role_title']\n",
    "    \n",
    "    # Multi-faceted search\n",
    "    searches = [\n",
    "        f\"{company} products technology stack {role}\",\n",
    "        f\"{company} recent news latest developments 2025 2026\",\n",
    "        f\"{company} company culture values engineering\"\n",
    "    ]\n",
    "    \n",
    "    all_results = []\n",
    "    for query in searches:\n",
    "        results = search_web(query, max_results=3)\n",
    "        all_results.extend(results)\n",
    "    \n",
    "    # Synthesize with LLM\n",
    "    if all_results:\n",
    "        context = \"\\n\\n\".join([\n",
    "            f\"Title: {r.get('title', '')}\\nContent: {r.get('content', '')}\" \n",
    "            for r in all_results\n",
    "        ])\n",
    "    else:\n",
    "        context = \"No search results available. Use general knowledge.\"\n",
    "    \n",
    "    llm = get_llm(temperature=0.3)\n",
    "    \n",
    "    prompt = f\"\"\"You are a senior career coach preparing a candidate for an interview.\n",
    "\n",
    "Company: {company}\n",
    "Role: {role}\n",
    "\n",
    "Research context:\n",
    "{context}\n",
    "\n",
    "Create a concise company summary (200-300 words) covering:\n",
    "1. Industry & business model\n",
    "2. Key products/services\n",
    "3. Technology stack (if relevant to {role})\n",
    "4. Recent news or developments\n",
    "5. Culture & values signals\n",
    "\n",
    "Then provide 5-8 specific talking points the candidate should be ready to reference.\n",
    "\n",
    "Format:\n",
    "# Company Summary\n",
    "[summary here]\n",
    "\n",
    "# Talking Points\n",
    "- [point 1]\n",
    "- [point 2]\n",
    "...\n",
    "\"\"\"\n",
    "    \n",
    "    response = llm.invoke([HumanMessage(content=prompt)])\n",
    "    output = response.content\n",
    "    \n",
    "    # Parse output\n",
    "    parts = output.split(\"# Talking Points\")\n",
    "    summary = parts[0].replace(\"# Company Summary\", \"\").strip()\n",
    "    \n",
    "    talking_points = []\n",
    "    if len(parts) > 1:\n",
    "        points_text = parts[1].strip()\n",
    "        talking_points = [\n",
    "            line.strip(\"- \").strip() \n",
    "            for line in points_text.split(\"\\n\") \n",
    "            if line.strip().startswith(\"-\")\n",
    "        ]\n",
    "    \n",
    "    print(\"\\nüìÑ Summary:\")\n",
    "    print(summary[:200] + \"...\")\n",
    "    print(f\"\\nüí° Generated {len(talking_points)} talking points\")\n",
    "    \n",
    "    return {\n",
    "        \"company_summary\": summary,\n",
    "        \"talking_points\": talking_points\n",
    "    }\n",
    "\n",
    "print(\"‚úÖ Company research node ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876d6ff0",
   "metadata": {},
   "source": [
    "## üë§ Section 6: Node - Research Interviewer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05fa36ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def research_interviewer_node(state: InterviewState) -> Dict[str, Any]:\n",
    "    \"\"\"Research interviewer background and create persona\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üë§ RESEARCHING INTERVIEWER\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    interviewer = state.get('interviewer_name') or \"Generic interviewer\"\n",
    "    company = state['company_name']\n",
    "    role = state['role_title']\n",
    "    \n",
    "    all_results = []\n",
    "    \n",
    "    if interviewer and interviewer.lower() not in [\"generic\", \"unknown\", \"na\", \"n/a\", \"\"]:\n",
    "        # Search for specific interviewer\n",
    "        searches = [\n",
    "            f\"{interviewer} {company} LinkedIn\",\n",
    "            f\"{interviewer} GitHub publications talks\",\n",
    "            f\"{interviewer} {company} engineering\"\n",
    "        ]\n",
    "        \n",
    "        for query in searches:\n",
    "            results = search_web(query, max_results=2)\n",
    "            all_results.extend(results)\n",
    "    \n",
    "    # Synthesize persona\n",
    "    if all_results:\n",
    "        context = \"\\n\\n\".join([\n",
    "            f\"Title: {r.get('title', '')}\\nContent: {r.get('content', '')}\" \n",
    "            for r in all_results\n",
    "        ])\n",
    "    else:\n",
    "        context = f\"No specific information found. Infer a realistic persona for a {role} interviewer at {company}.\"\n",
    "    \n",
    "    llm = get_llm(temperature=0.4)\n",
    "    \n",
    "    prompt = f\"\"\"You are a senior career coach analyzing an interviewer profile.\n",
    "\n",
    "Interviewer: {interviewer}\n",
    "Company: {company}\n",
    "Role being interviewed for: {role}\n",
    "\n",
    "Research context:\n",
    "{context}\n",
    "\n",
    "Create:\n",
    "1. A concise persona description (50-100 words): likely title, team, background, interests\n",
    "2. 5-7 specific hypotheses about what this interviewer will care about\n",
    "\n",
    "Format each hypothesis as:\n",
    "- \"Likely to probe [specific area] because [reason]\" OR\n",
    "- \"Red flag if candidate doesn't mention [specific thing]\"\n",
    "\n",
    "Format:\n",
    "# Persona\n",
    "[description]\n",
    "\n",
    "# Hypotheses\n",
    "- [hypothesis 1]\n",
    "- [hypothesis 2]\n",
    "...\n",
    "\"\"\"\n",
    "    \n",
    "    response = llm.invoke([HumanMessage(content=prompt)])\n",
    "    output = response.content\n",
    "    \n",
    "    # Parse output\n",
    "    parts = output.split(\"# Hypotheses\")\n",
    "    persona = parts[0].replace(\"# Persona\", \"\").strip()\n",
    "    \n",
    "    hypotheses = []\n",
    "    if len(parts) > 1:\n",
    "        hypotheses_text = parts[1].strip()\n",
    "        hypotheses = [\n",
    "            line.strip(\"- \").strip() \n",
    "            for line in hypotheses_text.split(\"\\n\") \n",
    "            if line.strip().startswith(\"-\")\n",
    "        ]\n",
    "    \n",
    "    print(\"\\nüìã Persona:\")\n",
    "    print(persona)\n",
    "    print(f\"\\nüéØ Generated {len(hypotheses)} interviewer hypotheses\")\n",
    "    \n",
    "    return {\n",
    "        \"interviewer_persona\": persona,\n",
    "        \"interviewer_hypotheses\": hypotheses\n",
    "    }\n",
    "\n",
    "print(\"‚úÖ Interviewer research node ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830ec140",
   "metadata": {},
   "source": [
    "## ‚ùì Section 7: Node - Generate Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c213124",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_questions_node(state: InterviewState) -> Dict[str, Any]:\n",
    "    \"\"\"Generate targeted interview questions\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"‚ùì GENERATING INTERVIEW QUESTIONS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Seniority-specific instructions\n",
    "    seniority_guide = {\n",
    "        \"junior\": \"Focus on learning agility, collaboration, foundational technical knowledge, and growth mindset.\",\n",
    "        \"mid\": \"Balance technical depth with ownership, cross-team collaboration, and independent problem-solving.\",\n",
    "        \"senior\": \"Emphasize system design, trade-offs, impact metrics, mentorship, and handling ambiguity.\",\n",
    "        \"lead\": \"Prioritize architectural decisions, organizational impact, technical leadership, and strategic thinking.\"\n",
    "    }\n",
    "    \n",
    "    seniority = state['seniority_level']\n",
    "    interview_type = state['interview_type']\n",
    "    \n",
    "    # Build context\n",
    "    company_context = state.get('company_summary', 'No company summary available')\n",
    "    interviewer_context = state.get('interviewer_persona', 'Generic interviewer')\n",
    "    job_desc = state.get('job_description', 'Not provided')\n",
    "    candidate_bg = state.get('candidate_background', 'Not provided')\n",
    "    \n",
    "    llm = get_llm(temperature=0.8)\n",
    "    \n",
    "    prompt = f\"\"\"You are an expert interview question designer for {state['role_title']} at {state['company_name']}.\n",
    "\n",
    "CONTEXT:\n",
    "Company: {company_context}\n",
    "\n",
    "Interviewer: {interviewer_context}\n",
    "\n",
    "Seniority: {seniority} - {seniority_guide[seniority]}\n",
    "\n",
    "Interview Type: {interview_type}\n",
    "\n",
    "Job Description: {job_desc}\n",
    "\n",
    "Candidate Background: {candidate_bg}\n",
    "\n",
    "TASK:\n",
    "Generate 10-15 interview questions following this distribution:\n",
    "- 40-60% behavioral (STAR-style questions about past experiences)\n",
    "- 20-40% role-specific technical (system design, coding concepts, architecture)\n",
    "- At least 2 \"curveball\" or meta questions (trade-offs, failure stories, ethical dilemmas)\n",
    "\n",
    "REQUIREMENTS:\n",
    "‚úì Tie questions to the company's domain and tech stack when possible\n",
    "‚úì Match the interviewer's likely concerns and background\n",
    "‚úì Calibrate difficulty to {seniority} level\n",
    "‚úì Avoid generic questions - make them specific and targeted\n",
    "‚úì Ensure diversity - no repetitive patterns\n",
    "\n",
    "Format as numbered list:\n",
    "1. [Question text]\n",
    "2. [Question text]\n",
    "...\n",
    "\"\"\"\n",
    "    \n",
    "    response = llm.invoke([HumanMessage(content=prompt)])\n",
    "    output = response.content\n",
    "    \n",
    "    # Parse numbered questions\n",
    "    questions = []\n",
    "    for line in output.split(\"\\n\"):\n",
    "        line = line.strip()\n",
    "        # Match patterns like \"1.\" or \"1)\" at start\n",
    "        if line and len(line) > 3:\n",
    "            if line[0].isdigit() and line[1:3] in ['. ', ') ']:\n",
    "                question = line.split('. ', 1)[-1] if '. ' in line else line.split(') ', 1)[-1]\n",
    "                questions.append(question.strip())\n",
    "    \n",
    "    print(f\"\\n‚ú® Generated {len(questions)} questions\")\n",
    "    print(\"\\nFirst 3 questions:\")\n",
    "    for i, q in enumerate(questions[:3], 1):\n",
    "        print(f\"  {i}. {q[:80]}...\")\n",
    "    \n",
    "    return {\"questions\": questions}\n",
    "\n",
    "print(\"‚úÖ Question generation node ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f84e74",
   "metadata": {},
   "source": [
    "## üé§ Section 8: Node - Interviewer Bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358ca374",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interviewer_bot_node(state: InterviewState) -> Dict[str, Any]:\n",
    "    \"\"\"Ask questions and provide feedback\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üé§ INTERVIEWER BOT\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    questions = state['questions']\n",
    "    current_idx = state['current_question_index']\n",
    "    conversation = state['conversation_history']\n",
    "    coverage = state['coverage_tracker'].copy()\n",
    "    \n",
    "    # Check if last turn needs feedback\n",
    "    if conversation and conversation[-1].get('answer') and not conversation[-1].get('score'):\n",
    "        print(\"\\nüìä Analyzing your answer...\")\n",
    "        \n",
    "        last_entry = conversation[-1]\n",
    "        question = last_entry['question']\n",
    "        answer = last_entry['answer']\n",
    "        \n",
    "        # Check for coach mode request\n",
    "        coach_triggers = ['how could i', 'better structure', 'feedback on', 'improve this', 'tell me how']\n",
    "        is_coach_request = any(trigger in answer.lower() for trigger in coach_triggers)\n",
    "        \n",
    "        llm = get_llm(temperature=0.4)\n",
    "        \n",
    "        if is_coach_request or state.get('coach_mode_active', False):\n",
    "            # Coach mode - metacognitive feedback\n",
    "            prompt = f\"\"\"You are a supportive interview coach in coaching mode.\n",
    "\n",
    "Question: {question}\n",
    "Candidate's response: {answer}\n",
    "\n",
    "The candidate is asking for structural guidance. Provide:\n",
    "1. What they did well\n",
    "2. Specific suggestions for improving their answer structure (use STAR if applicable)\n",
    "3. What additional details would strengthen the impact\n",
    "\n",
    "Be encouraging and specific. Don't score - just coach.\n",
    "\"\"\"\n",
    "            response = llm.invoke([HumanMessage(content=prompt)])\n",
    "            feedback = response.content\n",
    "            \n",
    "            print(\"\\nüí¨ COACH FEEDBACK:\")\n",
    "            print(feedback)\n",
    "            print(\"\\n[Returning to interviewer mode for next question]\\n\")\n",
    "            \n",
    "            conversation[-1]['coach_feedback'] = feedback\n",
    "            \n",
    "            return {\n",
    "                \"conversation_history\": conversation,\n",
    "                \"coach_mode_active\": False\n",
    "            }\n",
    "        else:\n",
    "            # Normal interviewer mode - score and rewrite\n",
    "            prompt = f\"\"\"You are an experienced interviewer evaluating a candidate's answer.\n",
    "\n",
    "Question: {question}\n",
    "Candidate's answer: {answer}\n",
    "Role: {state['role_title']}\n",
    "Seniority: {state['seniority_level']}\n",
    "\n",
    "Provide two sections:\n",
    "\n",
    "## Score & Signal\n",
    "- Score: [0-5]\n",
    "- What this signals to an experienced interviewer\n",
    "- Impact on hire confidence (increases/decreases/neutral)\n",
    "\n",
    "## Rewrite to A+\n",
    "A concise, improved version using STAR format where applicable, preserving the candidate's actual experience but structuring it optimally.\n",
    "\"\"\"\n",
    "            \n",
    "            response = llm.invoke([HumanMessage(content=prompt)])\n",
    "            feedback = response.content\n",
    "            \n",
    "            # Parse score\n",
    "            score = 3  # default\n",
    "            if \"Score:\" in feedback:\n",
    "                try:\n",
    "                    score_line = [l for l in feedback.split(\"\\n\") if \"Score:\" in l][0]\n",
    "                    score = int(score_line.split(\"Score:\")[1].strip()[0])\n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "            # Parse sections\n",
    "            parts = feedback.split(\"## Rewrite to A+\")\n",
    "            signal = parts[0].replace(\"## Score & Signal\", \"\").strip()\n",
    "            rewrite = parts[1].strip() if len(parts) > 1 else \"N/A\"\n",
    "            \n",
    "            print(\"\\nüìä SCORE & SIGNAL:\")\n",
    "            print(signal)\n",
    "            print(\"\\n‚ú® REWRITE TO A+:\")\n",
    "            print(rewrite)\n",
    "            \n",
    "            # Update coverage tracker\n",
    "            coverage_prompt = f\"\"\"Analyze this interview answer and rate 0-100 how much it demonstrates each dimension:\n",
    "\n",
    "Answer: {answer}\n",
    "\n",
    "Dimensions:\n",
    "- problem_solving\n",
    "- collaboration\n",
    "- ambiguity_handling\n",
    "- leadership_ownership\n",
    "- technical_depth\n",
    "\n",
    "Return ONLY a JSON object like: {{\"problem_solving\": 60, \"collaboration\": 20, ...}}\n",
    "\"\"\"\n",
    "            \n",
    "            try:\n",
    "                cov_response = llm.invoke([HumanMessage(content=coverage_prompt)])\n",
    "                cov_text = cov_response.content.strip()\n",
    "                # Extract JSON from markdown code blocks if present\n",
    "                if \"```\" in cov_text:\n",
    "                    cov_text = cov_text.split(\"```\")[1].replace(\"json\", \"\").strip()\n",
    "                cov_update = json.loads(cov_text)\n",
    "                # Update coverage (take max)\n",
    "                for dim, val in cov_update.items():\n",
    "                    if dim in coverage:\n",
    "                        coverage[dim] = max(coverage[dim], val / 100.0)\n",
    "            except Exception as e:\n",
    "                print(f\"  ‚ö†Ô∏è  Coverage update failed: {e}\")\n",
    "            \n",
    "            conversation[-1].update({\n",
    "                'score': score,\n",
    "                'signal': signal,\n",
    "                'rewrite': rewrite\n",
    "            })\n",
    "    \n",
    "    # Ask next question\n",
    "    if current_idx < len(questions):\n",
    "        next_question = questions[current_idx]\n",
    "        print(f\"\\n\\n‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\")\n",
    "        print(f\"Question {current_idx + 1}/{len(questions)}:\")\n",
    "        print(f\"‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\")\n",
    "        print(f\"\\n{next_question}\\n\")\n",
    "    else:\n",
    "        # Generate follow-up based on shallow answers\n",
    "        shallow_answers = [c for c in conversation if c.get('score', 5) < 3]\n",
    "        \n",
    "        if shallow_answers:\n",
    "            print(\"\\nüîÑ Generating follow-up question...\")\n",
    "            llm = get_llm(temperature=0.7)\n",
    "            \n",
    "            context = \"\\n\".join([\n",
    "                f\"Q: {a['question']}\\nA: {a['answer'][:100]}...\" \n",
    "                for a in shallow_answers[-2:]\n",
    "            ])\n",
    "            \n",
    "            prompt = f\"\"\"Based on these shallow/incomplete answers, generate ONE probing follow-up question:\n",
    "\n",
    "{context}\n",
    "\n",
    "Generate a question that digs deeper into an area the candidate avoided or glossed over.\n",
    "\"\"\"\n",
    "            response = llm.invoke([HumanMessage(content=prompt)])\n",
    "            next_question = response.content.strip()\n",
    "            \n",
    "            print(f\"\\n\\n‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\")\n",
    "            print(f\"Follow-up Question:\")\n",
    "            print(f\"‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\")\n",
    "            print(f\"\\n{next_question}\\n\")\n",
    "        else:\n",
    "            next_question = None\n",
    "    \n",
    "    if next_question:\n",
    "        conversation.append({\n",
    "            'question': next_question,\n",
    "            'answer': None,\n",
    "            'score': None,\n",
    "            'signal': None,\n",
    "            'rewrite': None\n",
    "        })\n",
    "        \n",
    "        return {\n",
    "            \"conversation_history\": conversation,\n",
    "            \"current_question_index\": current_idx + 1,\n",
    "            \"coverage_tracker\": coverage\n",
    "        }\n",
    "    else:\n",
    "        # No more questions - signal completion\n",
    "        return {\n",
    "            \"conversation_history\": conversation,\n",
    "            \"current_question_index\": current_idx + 1,\n",
    "            \"coverage_tracker\": coverage,\n",
    "            \"stop_requested\": True\n",
    "        }\n",
    "\n",
    "print(\"‚úÖ Interviewer bot node ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52859080",
   "metadata": {},
   "source": [
    "## üí¨ Section 9: Node - Get Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdcb410e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response_node(state: InterviewState) -> Dict[str, Any]:\n",
    "    \"\"\"Capture user's answer\"\"\"\n",
    "    conversation = state['conversation_history']\n",
    "    \n",
    "    if not conversation or conversation[-1].get('answer'):\n",
    "        # No pending question\n",
    "        return {}\n",
    "    \n",
    "    print(\"\\n\" + \"‚îÄ\"*60)\n",
    "    print(\"Your answer (type 'stop', 'end', or 'finish' to conclude):\")\n",
    "    print(\"‚îÄ\"*60)\n",
    "    \n",
    "    user_input = input(\"\\n‚Üí \")\n",
    "    \n",
    "    # Check for stop tokens\n",
    "    stop_tokens = ['stop', 'end', 'finish', 'quit', 'exit']\n",
    "    if user_input.lower().strip() in stop_tokens:\n",
    "        print(\"\\nüõë Interview concluded by user.\")\n",
    "        return {\"stop_requested\": True}\n",
    "    \n",
    "    # Record answer\n",
    "    conversation[-1]['answer'] = user_input\n",
    "    \n",
    "    return {\n",
    "        \"conversation_history\": conversation\n",
    "    }\n",
    "\n",
    "print(\"‚úÖ Get response node ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ccf63e",
   "metadata": {},
   "source": [
    "## üîÄ Section 10: Graph Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a6541a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the workflow graph\n",
    "def should_continue(state: InterviewState) -> Literal[\"get_response\", \"__end__\"]:\n",
    "    \"\"\"Route from interviewer_bot: continue or end\"\"\"\n",
    "    if state.get('stop_requested', False):\n",
    "        return \"__end__\"\n",
    "    \n",
    "    # Also end if we've done 20+ questions\n",
    "    if state.get('current_question_index', 0) > 20:\n",
    "        return \"__end__\"\n",
    "    \n",
    "    # Check if there's a pending question\n",
    "    conversation = state.get('conversation_history', [])\n",
    "    if conversation and conversation[-1].get('answer') is None:\n",
    "        return \"get_response\"\n",
    "    \n",
    "    return \"__end__\"\n",
    "\n",
    "# Create graph\n",
    "workflow = StateGraph(InterviewState)\n",
    "\n",
    "# Add nodes\n",
    "workflow.add_node(\"research_company\", research_company_node)\n",
    "workflow.add_node(\"research_interviewer\", research_interviewer_node)\n",
    "workflow.add_node(\"generate_questions\", generate_questions_node)\n",
    "workflow.add_node(\"interviewer_bot\", interviewer_bot_node)\n",
    "workflow.add_node(\"get_response\", get_response_node)\n",
    "\n",
    "# Add edges\n",
    "workflow.add_edge(START, \"research_company\")\n",
    "workflow.add_edge(\"research_company\", \"research_interviewer\")\n",
    "workflow.add_edge(\"research_interviewer\", \"generate_questions\")\n",
    "workflow.add_edge(\"generate_questions\", \"interviewer_bot\")\n",
    "\n",
    "# Conditional routing from interviewer_bot\n",
    "workflow.add_conditional_edges(\n",
    "    \"interviewer_bot\",\n",
    "    should_continue,\n",
    "    {\n",
    "        \"get_response\": \"get_response\",\n",
    "        \"__end__\": END\n",
    "    }\n",
    ")\n",
    "\n",
    "# Loop back from get_response\n",
    "workflow.add_edge(\"get_response\", \"interviewer_bot\")\n",
    "\n",
    "# Compile with checkpointing\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)\n",
    "\n",
    "print(\"‚úÖ LangGraph workflow compiled!\")\n",
    "print(\"\\nüìä Workflow structure:\")\n",
    "print(\"   START ‚Üí research_company ‚Üí research_interviewer ‚Üí generate_questions\")\n",
    "print(\"         ‚Üí interviewer_bot ‚Üî get_response ‚Üí END\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1676a963",
   "metadata": {},
   "source": [
    "## üìù Section 11: Final Report Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8966e553",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_final_report(state: InterviewState) -> str:\n",
    "    \"\"\"Generate comprehensive final report\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üìù GENERATING FINAL REPORT\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    conversation = state['conversation_history']\n",
    "    coverage = state['coverage_tracker']\n",
    "    \n",
    "    # Build conversation summary\n",
    "    convo_summary = \"\\n\\n\".join([\n",
    "        f\"Q: {c['question']}\\nA: {c.get('answer', 'N/A')[:200]}...\\nScore: {c.get('score', 'N/A')}\" \n",
    "        for c in conversation[:10]  # Limit context\n",
    "    ])\n",
    "    \n",
    "    coverage_summary = \"\\n\".join([\n",
    "        f\"- {dim.replace('_', ' ').title()}: {int(score*100)}%\" \n",
    "        for dim, score in coverage.items()\n",
    "    ])\n",
    "    \n",
    "    llm = get_llm(temperature=0.5)\n",
    "    \n",
    "    prompt = f\"\"\"You are a senior interview coach providing a final debrief.\n",
    "\n",
    "Interview for: {state['role_title']} at {state['company_name']}\n",
    "Seniority: {state['seniority_level']}\n",
    "Questions asked: {len(conversation)}\n",
    "\n",
    "Coverage achieved:\n",
    "{coverage_summary}\n",
    "\n",
    "Sample conversation:\n",
    "{convo_summary}\n",
    "\n",
    "Generate a comprehensive final report with:\n",
    "\n",
    "## üí™ Strengths\n",
    "3-5 bullet points of what the candidate did well\n",
    "\n",
    "## ‚ö†Ô∏è  Risk Areas\n",
    "2-4 bullet points of areas that need improvement\n",
    "\n",
    "## üìö Stories to Refine\n",
    "3-5 specific examples/stories the candidate should polish, with what's missing (STAR gaps)\n",
    "\n",
    "## üéØ 3-Session Practice Plan\n",
    "Week-by-week focus areas:\n",
    "- Session 1: [focus]\n",
    "- Session 2: [focus]\n",
    "- Session 3: [focus]\n",
    "\n",
    "Be specific, actionable, and encouraging.\n",
    "\"\"\"\n",
    "    \n",
    "    response = llm.invoke([HumanMessage(content=prompt)])\n",
    "    report = response.content\n",
    "    \n",
    "    return report\n",
    "\n",
    "print(\"‚úÖ Final report generator ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbcf4def",
   "metadata": {},
   "source": [
    "## üíæ Section 12: Export/Import Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c4d9ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_session(state: InterviewState, filename: str = None):\n",
    "    \"\"\"Export session state to JSON\"\"\"\n",
    "    if filename is None:\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        filename = f\"interview_session_{timestamp}.json\"\n",
    "    \n",
    "    # Convert state to JSON-serializable format\n",
    "    export_data = dict(state)\n",
    "    \n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(export_data, f, indent=2)\n",
    "    \n",
    "    print(f\"\\nüíæ Session exported to: {filename}\")\n",
    "    \n",
    "    if IN_COLAB:\n",
    "        files.download(filename)\n",
    "        print(\"   File download started...\")\n",
    "    \n",
    "    return filename\n",
    "\n",
    "def import_session(filename: str) -> InterviewState:\n",
    "    \"\"\"Import session state from JSON\"\"\"\n",
    "    with open(filename, 'r') as f:\n",
    "        state = json.load(f)\n",
    "    \n",
    "    print(f\"‚úÖ Session imported from: {filename}\")\n",
    "    return state\n",
    "\n",
    "print(\"‚úÖ Export/import utilities ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8901b719",
   "metadata": {},
   "source": [
    "## üöÄ Section 13: Main Execution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82cdda7c",
   "metadata": {},
   "source": [
    "### Initialize Interview Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ab0199",
   "metadata": {},
   "outputs": [],
   "source": [
    "# User inputs\n",
    "print(\"üéØ Interview Setup\\n\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "COMPANY_NAME = input(\"Company name: \").strip() or \"Google\"\n",
    "ROLE_TITLE = input(\"Role title: \").strip() or \"Senior Software Engineer\"\n",
    "INTERVIEWER_NAME = input(\"Interviewer name (optional): \").strip() or None\n",
    "\n",
    "print(\"\\nSeniority level: 1) Junior  2) Mid  3) Senior  4) Lead\")\n",
    "seniority_choice = input(\"Choose (1-4): \").strip() or \"3\"\n",
    "SENIORITY_LEVEL = {\"1\": \"junior\", \"2\": \"mid\", \"3\": \"senior\", \"4\": \"lead\"}.get(seniority_choice, \"mid\")\n",
    "\n",
    "print(\"\\nInterview type: 1) Behavioral  2) System Design  3) Coding  4) Mixed\")\n",
    "type_choice = input(\"Choose (1-4): \").strip() or \"4\"\n",
    "INTERVIEW_TYPE = {\"1\": \"behavioral\", \"2\": \"system-design\", \"3\": \"coding\", \"4\": \"mixed\"}.get(type_choice, \"mixed\")\n",
    "\n",
    "JOB_DESCRIPTION = input(\"\\nJob description (optional, paste or skip): \").strip() or None\n",
    "CANDIDATE_BACKGROUND = input(\"Your background summary (optional): \").strip() or None\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ Configuration complete!\")\n",
    "print(f\"   Company: {COMPANY_NAME}\")\n",
    "print(f\"   Role: {ROLE_TITLE}\")\n",
    "print(f\"   Seniority: {SENIORITY_LEVEL}\")\n",
    "print(f\"   Type: {INTERVIEW_TYPE}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2f7274",
   "metadata": {},
   "source": [
    "### Start Interview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7faebab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create initial state\n",
    "initial_state = create_initial_state(\n",
    "    company_name=COMPANY_NAME,\n",
    "    role_title=ROLE_TITLE,\n",
    "    seniority_level=SENIORITY_LEVEL,\n",
    "    interview_type=INTERVIEW_TYPE,\n",
    "    job_description=JOB_DESCRIPTION,\n",
    "    interviewer_name=INTERVIEWER_NAME,\n",
    "    candidate_background=CANDIDATE_BACKGROUND\n",
    ")\n",
    "\n",
    "# Run workflow with thread for checkpointing\n",
    "config = {\"configurable\": {\"thread_id\": \"interview_session_1\"}}\n",
    "\n",
    "print(\"\\nüöÄ Starting interview workflow...\\n\")\n",
    "\n",
    "# Execute interview loop\n",
    "final_state = None\n",
    "for output in app.stream(initial_state, config):\n",
    "    # Stream processes each node; output is dict with node name as key\n",
    "    pass\n",
    "\n",
    "# Get final state from last checkpoint\n",
    "final_state = app.get_state(config).values\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üéâ INTERVIEW COMPLETE\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e99933",
   "metadata": {},
   "source": [
    "### Generate & Display Final Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff3fa01",
   "metadata": {},
   "outputs": [],
   "source": [
    "if final_state:\n",
    "    final_report = generate_final_report(final_state)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üìä FINAL REPORT\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"\\n\" + final_report)\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    \n",
    "    # Update state with report\n",
    "    final_state['final_report'] = final_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e31f80b",
   "metadata": {},
   "source": [
    "### Export Session (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e7545e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the session\n",
    "if final_state:\n",
    "    export_choice = input(\"\\nExport session to JSON? (y/n): \").strip().lower()\n",
    "    if export_choice == 'y':\n",
    "        export_session(final_state)\n",
    "    else:\n",
    "        print(\"\\nüìù Session not exported. You can export later by calling export_session(final_state)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7efc0d1d",
   "metadata": {},
   "source": [
    "## üìä Section 14: Session Analytics (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e48db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display session statistics\n",
    "if final_state:\n",
    "    print(\"\\nüìà SESSION STATISTICS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    conversation = final_state.get('conversation_history', [])\n",
    "    coverage = final_state.get('coverage_tracker', {})\n",
    "    \n",
    "    # Question stats\n",
    "    answered = [c for c in conversation if c.get('answer')]\n",
    "    scored = [c for c in answered if c.get('score') is not None]\n",
    "    \n",
    "    print(f\"\\nQuestions:\")\n",
    "    print(f\"  Total generated: {len(final_state.get('questions', []))}\")\n",
    "    print(f\"  Asked: {len(conversation)}\")\n",
    "    print(f\"  Answered: {len(answered)}\")\n",
    "    \n",
    "    if scored:\n",
    "        avg_score = sum(c['score'] for c in scored) / len(scored)\n",
    "        print(f\"  Average score: {avg_score:.1f}/5\")\n",
    "    \n",
    "    print(f\"\\nCoverage:\")\n",
    "    for dim, score in coverage.items():\n",
    "        bar_length = int(score * 20)\n",
    "        bar = \"‚ñà\" * bar_length + \"‚ñë\" * (20 - bar_length)\n",
    "        print(f\"  {dim.replace('_', ' ').title():25} [{bar}] {int(score*100)}%\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2caddf7c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéì Usage Instructions\n",
    "\n",
    "### Quick Start:\n",
    "1. Run all cells in order (Runtime ‚Üí Run all)\n",
    "2. Enter API keys when prompted\n",
    "3. Configure your interview in Section 13\n",
    "4. Answer questions interactively\n",
    "5. Type `stop` to end early, or complete all questions\n",
    "6. Review your final report\n",
    "\n",
    "### Tips:\n",
    "- Use STAR format (Situation, Task, Action, Result) for behavioral questions\n",
    "- Type variations of \"how could I improve this\" to trigger coach mode during any answer\n",
    "- Export your session to track progress across multiple practice sessions\n",
    "- Review the \"Rewrite to A+\" sections to see optimal answer structure\n",
    "\n",
    "### Customization:\n",
    "- Modify `LLM_PROVIDER` and `MODEL_NAME` in Section 2 to switch between providers\n",
    "- Adjust question count in `generate_questions_node` prompt\n",
    "- Customize seniority guidance in Section 7\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
